Example of CICD based mutable deployment

Why Terraform is not required

Ansible could be used but simpler CICD pipelines achieve the same.

Ansible should still be used to manage the configurations and packages within the server

---

An ALB -> Target Group -> Auto-scaling group -> min-3 and max 10 ec2 defined

---

At this point all 10 ec2 are running with java app - version 1.10

--

you need to deploy version 1.11 of the java app

--

how will you do it?

---

How will the user data pull the latest version of my app which is 1.11

-- Assume that the new version is a jar file of version 1.11

-- the jar file is created as a part of CI -- mvn clean package - create my jar file with version 1.11

-- it is placed at some common repo / registry / artifact management tool (artifactory / nexus / azure artifacts)

-- you will transfer this jar.1.11 to a central repo 

-- add the updated path and version of the new jar file inside the user data

----

in the cases where e.g. 3 ec2 instance of type t4.8xlarge runs without auto-scaling...

simply opt of mutable deployment / in-pace upgrades..

ssh into the 3 ec2 servers -> download the jar.1.11 to a central repo -> place it inside the correct folder (/opt/app) -> restart you application (systemctl restart app)

--

in the cases where ec2 instances are controlled by auto-scaling groups then opt of immutable type of deployment

---

ec2 based auto-scaling... very common question?

sometime you have 2 instances and sometimes, you have 5 running..

when ever you have to deploy new application changes to them..

how will you do that...

---

anti-patterns:

--> I will make sure that my jenkins always watches for the number of ec2 running and make sure that new version is always deployed in it

--------

testing the new version:

running and testing it in lower env... dev -> qa -> stag/UAT -> prod


--- Immutable 

we can use Terraform but should be aware that if we update the ami id the entire instance will be re-created.. so catious


-------------------------------

music streaming app...

it is running on 12 ec2 instances... it is running java app..

We need to deploy latest version of our app in it...

there should not be any downtime, while deploying the application
existing java app version: 1.1

how will we do it considering both approaches:

mutable:
- remove from alb (how many servers you will remove the ALB at once?)
- one by one would be an overkill
- parallel update in all 12 will have a huge risk of production downtime
- best approach:
    - remove one server from target group first -> perform the updates -> test and validate -> if this is fine -> do it in bunch of 3/4 servers
- alternative approach:
    - create a virtual group of servers
        - server-group-A: 3 servers in it
        - server-group-B: 3 servers in it
        - server-group-C: 3 servers in it
        - server-group-D: 3 servers in it
- update the version to 1.2
- add it back in the alb

Additional Note: Whenever inside Kubernetes, you deploy a latest version of an app by updating the tag in the deployment.yaml file of that application and if it has 4 replicas running, check how kubernetes perform upgrade
maxSurge: Defines the maximum number of Pods that can be created above the desired number of replicas (4 in your case). If maxSurge is set to 1, Kubernetes will create one new Pod before terminating an old one.
maxUnavailable: Defines the maximum number of Pods that can be unavailable during the update process. If maxUnavailable is set to 1, Kubernetes will ensure at least 3 Pods are running at all times.

immutable: 
Blue / Green Deployment:
- existing target group with 12 ec2 running the java app on version (1.1) - Blue Env
- Create a new target group with 12 new ec2s with new java app version (1.2) - green env
- test everything in the green env, validate the user flows, business flows
- once the new env is success replace this target group as the main target group in ALB (promote the green to blue)

Current scenario:
ALB -> Target group (Blue)
    -> Target group (green)

if the changes in green env is successful:
ALB -> Target group (green)
    -> Target group (Blue)


In this approach there will be minor glitch/ downtime:
we will be running 2 commands:
1.) to remove the blue TG
2.) add the Green TG

at least 1/2 sec of downtime is expected

-----------

IF no downtime at all is desired:

Blue env: ALB + Target Group + 12 EC2

Green env: ALB + Target Group + 12 EC2

DNS level:

Route53: select appropriate routing strategy in the DNS record

Update the CNAME to Green env ALB DNS - non AWS native approach

Let us search if there is an AWS Route53 native approach for handling blue / green?


----
Database:

1st option:
- the green env has a separate DB altogether 
- the green db syncs data regularly from blue db
- when you are testing the changes on the green env, at that time as well the could be some write operations happing in the blue db... again you will have to re-sync that
- There are clustered Database options: those can be used here

General option:
- Same db for both blue and green env
- the team should make sure that all the changes applied in the db can be roll backed easily
- you make sure that tests are not impacting any live data

------

Canary (progressively releasing the changes from small % of users to 100% eventually):

1.) Scenario 1 - where we have fixed 20 instances.. you can not increase or decrease the instance count:

- All these 20 ec2s running behind an ALB... all serving live traffic

- now if you want to deploy the latest changes via the canary approach:

- you define your canary approach: 
    - New changes will be released to 5%
    - then 20%
    - then 50%
    - finally 100%

- pick any 1 server, apply the latest changes here (ssh into the server and apply the changes)
    - now out of 100 request 5 request will go to this server... 5% of the users gets to see the latest changes

- pick any 4 server, apply the latest changes here (ssh into the server and apply the changes)
    - now out of 100 request 20 request will go to this server... 20% of the users gets to see the latest changes

similarly do it for 100%

now what if you find issues after releasing it to 20% of the user base:
- again ssh into those 4 servers..
- copy the old binary pointing to the earlier version
- now all 100% of the servers are on the older release

----

achieving canary in kubernetes:
- argo rollouts
- istio or linkerd - service mesh

End to end Auto-Scaling in Kubernetes:
- Pods - HPA (Horizontal Pod Autoscaling)
- Node - Autoscalinggroup + ClusterAutoscalar

Why is cluster autoscalar required?
- checks for any pending pods in the cluster..
- then it send signal to autoscaling group

karpenter which can help in node auto-scaling and karpenter works without auto-scaling group.

----